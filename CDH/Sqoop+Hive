
1Sqoop数据导入与导出  
1 Sqoop 是一组工具集，包含了RDBS 与Hadoop 数据交互的多种工具。这些工具 主要包括等Import，Export
2 基本设置
$SQOOP_HOME
$HADOOP_HOME
3 Import  工具介绍 
#选择Sqoop Import工具
$ sqoop  import  -help 
#import 工具用于将数据导入到Hadoop平台中
#数据源一般选择JDBC连接
#Mysql：jdbc:mysql://localhost:3306/student
#Oracle：jdbc:oracle:thin:@//<host>:<port>/ServiceName或jdbc:oracle:thin:@<host>:<port>:<SID>
#其他需要账户名、密码
#设置数据连接信息
#可以选择多种密码安全措施
#数据源设置内容包括连接信息、账户名与密码、指定数据字段范围
#1数据源url
--connect jdbc:mysql://database.example.com/employees \
--username dbuser  \
--password-file ${user.home}/.password-alias  \
#2选择数据源有两种选择方式
#--------------------方法一--------------------------------
#通过制定多个参数
--table  ${tablename}  \
--columns "name,employee_id,jobtitle"  \
--where "id > 400"
#------------------方法二-----------------------------------  
#或者通过query的方式选择数据
--query 'SELECT a.*, b.* FROM a JOIN b on (a.id == b.id) WHERE $CONDITIONS' 
--split-by a.id --target-dir /user/foo/joinresults
#或者
--query “SELECT a.*, b.* FROM a JOIN b on (a.id == b.id) WHERE \$CONDITIONS”\
-m 1 --target-dir /user/foo/joinresults
#3指定其他参数
-m 1  或者  --split-by a.id 
--as-textfile 
#以上两种方式指定了平行执行的模式，-m  选择单个map执行，--split-by 执行了并行执行的split字段，当没有分割字段时，默认选择单个key作为分割字段。
#4指定target信息
#直接输出到目录
--target-dir /user/foo/joinresults
--input-fields-terminated-by  '|'
#指定hive 参数
--hive-import   --hive-table   test.demo

4 export 工具介绍
$ Sqoop export  -help
sqoop  export   --connect  jdbc:mysql://172.17.17.90:3306/test   \
--username root  --password  huawei  \
--export-dir  hdfs://cscloud-citus-mppdb-14.huawei.com:8020/user/hive/warehouse/test.db/demo     \
--table demo  \
--fields-terminated-by  "\001"
-m=1
#export 默认导入的是HDFS文件，模式是insert 。
#文件需要指定分隔符。
#若通过—cloumns  指定特定列，没有指定的列必须指定默认值或者mysql表允许这些字段为空。--input-null-string   或者 --input-null-non-string 
#若采用update模式，需要指定主键。

例子：
Oracle jdbc URL：jdbc:oracle:thin:@10.138.46.11:1521:testdb  （SID=testdb）
Mysql jdbc URL：jdbc:mysql://localhost:3306/easonjim

Sqoop Oracle 到HDFS 
$sqoop  import --connect jdbc:oracle:thin:@10.138.46.11:1521:testdb  \
 --username  tohdfs --password zR8nBJfeSOIYa0yu \
--table WO_WORKOREDR_SOURCE_INDEX \ 
--m 1   \
--target-dir  /tmp/oracle/date21  \
--as-textfile  \
--fields-terminated-by '\t'
Sqoop mysql 到HDFS 
 sqoop  import --connect   jdbc:mysql://aas01:3306/test1  \
--username  root  --password  123456  \
--table  test_to_hive  \
--target-dir  /tmp/acl/test_to_hive  
--m 1 
--as-textfile
--fields-terminated-by '\t'
Sqoop mysql 到Hive 
sqoop  import --connect   jdbc:mysql://aas01:3306/test1 \
--username  root  --password  123456  \
--table  test_to_hive   --m 1 --as-textfile  \
--hive-import   --create-hive-table \
--hive-table test_to_hive  --hive-database  test          
Sqoop hive 到Mysql 
sqoop  export --connect   jdbc:mysql://aas01:3306/test1  \
--username  root  --password  123456  \
--table  test_to_hive   \
--export-dir    /user/hive/warehouse/test.db/test_to_hive    --fields-terminated-by   '\001'
hive导出到mysql，实际上是将hdfs文件导出到mysql，所以需要指定文件的路径以及列分隔符；



HiveSQL  
#DDL 数据定义语言
1 hive建表
创建内部表
#create  table  demo2(
id int,
name  String
)
partitioned by (age int)
clustered by(id) into 3 buckets 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS PARQUET
创建外部表或临时表：
create [TEMPORARY] [EXTERNAL] table  demo4(
id int,
name  String,
age  int) 
ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ','
LOCATION  'hdfs://cscloud-citus-mppdb-14.huawei.com:8020/user/hive/warehouse/test.db/demo4'

2 Hive 创建View视图
create  view  view_demo  as  select * from demo;

3 Hive分区
#创建分区
#开启动态分区：
set hive.exec.dynamic.partition=true; 
set hive.exec.dynamic.partition.mode=nonstrict; 
#设置其他参数（可选）
SET hive.exec.max.dynamic.partitions=100000;
SET hive.exec.max.dynamic.partitions.pernode=100000;
set hive.enforce.bucketing = true;
4 数据插入Hive分区表
insert overwrite  table  demo2  partition (age)   select * from  demo where age is not null ;
inset into table demo_partition  partition(age=12) select *  from demo  where age=12;
5 数据库
# 删除数据库
drop  database   dbname   cascade；
cascade  是级联删除的意思。

---------DML
   
5 Alter 操作
#修改表名
alter  table demo rename  to demo2;
#修改字段类型
#修改字段名称
#添加新字段
alter column 
// First change column a's name to a1.
ALTER TABLE test_change CHANGE a a1 INT;
修改分区
Alter partition 
#插入分区
ALTER TABLE page_view ADD PARTITION (dt='2008-08-08', country='us') location '/path/to/us/part080808'
                          PARTITION (dt='2008-08-09', country='us') location '/path/to/us/part080809';



问题集
1 mysql 数据库整库导入Hive；
2 Hive数据表导出到Sqoop；
3 Hive 建表（外部表与内部表），指定分隔符以及存储格式，分区分桶等
4 hive View 使用；
5 Alter 修改schema，包括表名、字段名、分区信息。
8 正则切分字符串；
9 行专列与列转行；


数据处理与操作
1 Hive实现列转行操作：
创建表studnet：
create table student(
    name string,
	subject string,
	score decimal(4,1))
row format delimited
fields terminated by '|';

数据文件及内容： student.txt
xiaoming|english|92.0
xiaoming|chinese|98.0
xiaoming|math|89.5
huahua|chinese|80.0
huahua|math|89.5

执行转换：
> select name,concat_ws(',',collect_set(subject)) from student group by name;
huahua    chinese,math
xiaoming  english,chinese,math

> select name,concat_ws(',',collect_set(concat(subject,'=',score))) from student group by name;
huahua   chinese=80,math=89.5
xiaoming english=92,chinese=98,math=89.5

2 Hive实现行转列操作
数据文件及内容：student2.txt
huahua|chinese=80,math=89.5
xiaoming|english=92,chinese=98,math=89.5

创建表：
create table student2(name string,subject_score_list string)
row format delimited
fields terminated by '|';

> select name, subject_list from student2 stu2 lateral view explode(split(stu2.subject_score_list,','))stu_subj as subject_list; ----别名一定不要忘记
huahua    chinese=80
huahua    math=89.5
xiaoming  english=92
xiaoming  chinese=98
xiaoming  math=89.5

3 case then 语句实例；
--简单case函数
case sex
  when '1' then '男'
  when '2' then '女’
  else '其他' end
--case搜索函数
case when sex = '1' then '男'
     when sex = '2' then '女'
     else '其他' end  

4 字符串操作方法
--字符串拼接
concat('foo', 'bar')  => 'foobar'
concat_ws(',','foo', 'bar') => 'foo,bar'
--返回index
instr(string str, string substr) 
0表示不包含，位置从1开始。
替换：replace（strA,strB） 
子串：substr(string|binary A, int start, int len) 
切分：split(string str, string pat)
大小写：upper(string A)，lower(string A) 
正则切分：

5 时间提取，年、月、日，系统时间戳获取，时间戳与日期格式转换；
获取日期：current_date
获取时间：current_timestamp
获取当前系统时间戳：unix_timestamp()
unix_timestamp()  获取时间戳；
unix_timestamp(string date, string pattern) 如unix_timestamp('2009-03-20', 'yyyy-MM-dd') = 1237532400. 
from_unixtime(bigint unixtime[, string format]) 时间戳转换为date类型，pattern如yyyy-MM-dd HH:mm:ss 

6 数据类型转换
cast(cast(a as string) as double)；



插入数据

其他函数：

insert  into /override
Sqoop 导入与导出



数据聚合分析部分：
这些函数中hive与Impala的区别。
常见的聚合函数，最大，最小，求和，平均，方差，标准差。
开窗函数topN，统计实例；
多种关联，leftjoin，join，inner join。
union 实例：union  与union all 
1 Windows 开窗函数
一般格式：
SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)
FROM T;
或者指定前后选取的行数：
UNBOUNDED PRECEDING指首行
UNBOUNDED FOLLOWING指尾行
CURRENT ROW当前行
3 PRECEDING 前3行 （不存在时将跳过）
3 FOLLOWING 后3行

2 与Windows 配合使用的分析函数：
#排序RANK（）
#行号ROW_NUMBER（）
#DENSE_RANK（）
CUME_DIST
PERCENT_RANK
#NTILE（）

参考链接；
https://www.cnblogs.com/52XF/p/4209211.html
#排序后按序号取数
#可产生序号，每行序号在分组内唯一；
#row_number（）按需排，相同的值也有不同的序号。
with orderSection as
(
    select ROW_NUMBER() OVER(order by [SubTime] desc) rownum,* from [Order]
)
select * from [orderSection] where rownum between 3 and 5 order by [SubTime] desc
#如小组内有三条记录（1,1,2），则rank（）得到的序号是：
（1,1）
（2,1）
（3,2）

#rank（）
#指排名，与row_number不同的是，它的相同值有相同的值。
#可用来生成排名
#如小组内有三条记录（1,1,2，3），则rank（）得到的序号是：
（1,1）
（1,1）
（3,2）
（4,3）
#dense_rank（）
#与rank类似，但是会产生连续的排名：
#如小组内有三条记录（1,1,2，3），则rank（）得到的序号是：
（1,1）
（1,1）
（2,2）
（3,3）
#NTILE(4)用于将查询的结果分桶，并显示桶的编号，从1开始；
select NTILE(4) OVER(order by [SubTime] desc) as ntile,* from [Order]






3  取topN
#lead(N) topN
SELECT a, LEAD(a) OVER (PARTITION BY b ORDER BY C)
FROM T;
#lag(N)  tail N 
SELECT a, LAG(a, 3, 0) OVER (PARTITION BY b ORDER BY C)
FROM T;





