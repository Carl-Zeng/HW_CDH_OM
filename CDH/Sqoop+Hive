
1Sqoop数据导入与导出  
1 Sqoop 是一组工具集，包含了RDBS 与Hadoop 数据交互的多种工具。这些工具 主要包括等Import，Export
2 基本设置
$SQOOP_HOME
$HADOOP_HOME
3 Import  工具介绍 
#选择Sqoop Import工具
$ sqoop  import  -help 
#import 工具用于将数据导入到Hadoop平台中
#数据源一般选择JDBC连接
#Mysql：jdbc:mysql://localhost:3306/student
#Oracle：jdbc:oracle:thin:@//<host>:<port>/ServiceName或jdbc:oracle:thin:@<host>:<port>:<SID>
#其他需要账户名、密码
#设置数据连接信息
#可以选择多种密码安全措施
#数据源设置内容包括连接信息、账户名与密码、指定数据字段范围
#1数据源url
--connect jdbc:mysql://database.example.com/employees \
--username dbuser  \
--password-file ${user.home}/.password-alias  \
#2选择数据源有两种选择方式
#--------------------方法一--------------------------------
#通过制定多个参数
--table  ${tablename}  \
--columns "name,employee_id,jobtitle"  \
--where "id > 400"
#------------------方法二-----------------------------------  
#或者通过query的方式选择数据
--query 'SELECT a.*, b.* FROM a JOIN b on (a.id == b.id) WHERE $CONDITIONS' 
--split-by a.id --target-dir /user/foo/joinresults
#或者
--query “SELECT a.*, b.* FROM a JOIN b on (a.id == b.id) WHERE \$CONDITIONS”\
-m 1 --target-dir /user/foo/joinresults
#3指定其他参数
-m 1  或者  --split-by a.id 
--as-textfile 
#以上两种方式指定了平行执行的模式，-m  选择单个map执行，--split-by 执行了并行执行的split字段，当没有分割字段时，默认选择单个key作为分割字段。
#4指定target信息
#直接输出到目录
--target-dir /user/foo/joinresults
--input-fields-terminated-by  '|'
#指定hive 参数
--hive-import   --hive-table   test.demo

4 export 工具介绍
$ Sqoop export  -help
sqoop  export   --connect  jdbc:mysql://172.17.17.90:3306/test   \
--username root  --password  huawei  \
--export-dir  hdfs://cscloud-citus-mppdb-14.huawei.com:8020/user/hive/warehouse/test.db/demo     \
--table demo  \
--fields-terminated-by  "\001"
-m=1
#export 默认导入的是HDFS文件，模式是insert 。
#文件需要指定分隔符。
#若通过—cloumns  指定特定列，没有指定的列必须指定默认值或者mysql表允许这些字段为空。--input-null-string   或者 --input-null-non-string 
#若采用update模式，需要指定主键。

例子：
Oracle jdbc URL：jdbc:oracle:thin:@10.138.46.11:1521:testdb  （SID=testdb）
Mysql jdbc URL：jdbc:mysql://localhost:3306/easonjim

Sqoop Oracle 到HDFS 
$sqoop  import --connect jdbc:oracle:thin:@10.138.46.11:1521:testdb  \
 --username  tohdfs --password zR8nBJfeSOIYa0yu \
--table WO_WORKOREDR_SOURCE_INDEX \ 
--m 1   \
--target-dir  /tmp/oracle/date21  \
--as-textfile  \
--fields-terminated-by '\t'
Sqoop mysql 到HDFS 
 sqoop  import --connect   jdbc:mysql://aas01:3306/test1  \
--username  root  --password  123456  \
--table  test_to_hive  \
--target-dir  /tmp/acl/test_to_hive  
--m 1 
--as-textfile
--fields-terminated-by '\t'
Sqoop mysql 到Hive 
sqoop  import --connect   jdbc:mysql://aas01:3306/test1 \
--username  root  --password  123456  \
--table  test_to_hive   --m 1 --as-textfile  \
--hive-import   --create-hive-table \
--hive-table test_to_hive  --hive-database  test          
Sqoop hive 到Mysql 
sqoop  export --connect   jdbc:mysql://aas01:3306/test1  \
--username  root  --password  123456  \
--table  test_to_hive   \
--export-dir    /user/hive/warehouse/test.db/test_to_hive    --fields-terminated-by   '\001'
hive导出到mysql，实际上是将hdfs文件导出到mysql，所以需要指定文件的路径以及列分隔符；







2	HiveSQL  
#DDL 数据定义语言
1 hive建表
创建内部表
#create  table  demo2(
id int,
name  String
)
partitioned by (age int)
clustered by(id) into 3 buckets 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS PARQUET
创建外部表或临时表：
create [TEMPORARY] [EXTERNAL] table  demo4(
id int,
name  String,
age  int) 
ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ','
LOCATION  'hdfs://cscloud-citus-mppdb-14.huawei.com:8020/user/hive/warehouse/test.db/demo4'
2 Hive 创建View视图



3	Hive分区
1 创建分区
默认为静态分区模式，静态分区模式下必须显式的指定分区字段值，动态分区模式下，可以动态指定分区指。
开启动态分区：
set hive.exec.dynamic.partition=true;(可通过这个语句查看：set hive.exec.dynamic.partition;) 
set hive.exec.dynamic.partition.mode=nonstrict; 
#设置其他参数（可选）
SET hive.exec.max.dynamic.partitions=100000;(如果自动分区数大于这个参数，将会报错)
SET hive.exec.max.dynamic.partitions.pernode=100000;
set hive.enforce.bucketing = true;强制分桶。
2 数据插入Hive分区表
insert overwrite  table  demo2  partition (age)   select * from  demo where age is not null ;

3 数据库
1 删除数据库
drop  database   dbname   cascade；
cascade  是级联删除的意思。




---------DML
  



4 
5 Alter 操作
alter column 
// First change column a's name to a1.
ALTER TABLE test_change CHANGE a a1 INT;
Alter partition 
ALTER TABLE page_view ADD PARTITION (dt='2008-08-08', country='us') location '/path/to/us/part080808'
                          PARTITION (dt='2008-08-09', country='us') location '/path/to/us/part080809';

DDL
alter 修改 表明，字段名，添加字段，修改字段类型；
创建分区表：包括数据插入静态分区与动态分区；
分桶；
create table like 与 as；









问题集
1 mysql 数据库整库导入Hive；
2 Hive数据表导出到Sqoop；
3 Hive 建表（外部表与内部表），指定分隔符以及存储格式，分区分桶等
4 hive View 使用；
5 Alter 修改schema，包括表名、字段名、分区信息。
8 正则切分字符串；
9 行专列与列转行；


数据处理与操作
1 Hive实现列转行操作：
创建表studnet：
create table student(
    name string,
	subject string,
	score decimal(4,1))
row format delimited
fields terminated by '|';

数据文件及内容： student.txt
xiaoming|english|92.0
xiaoming|chinese|98.0
xiaoming|math|89.5
huahua|chinese|80.0
huahua|math|89.5

执行转换：
> select name,concat_ws(',',collect_set(subject)) from student group by name;
huahua    chinese,math
xiaoming  english,chinese,math

> select name,concat_ws(',',collect_set(concat(subject,'=',score))) from student group by name;
huahua   chinese=80,math=89.5
xiaoming english=92,chinese=98,math=89.5

2 Hive实现行转列操作
数据文件及内容：student2.txt
huahua|chinese=80,math=89.5
xiaoming|english=92,chinese=98,math=89.5

创建表：
create table student2(name string,subject_score_list string)
row format delimited
fields terminated by '|';

> select name, subject_list from student2 stu2 lateral view explode(split(stu2.subject_score_list,','))stu_subj as subject_list; ----别名一定不要忘记
huahua    chinese=80
huahua    math=89.5
xiaoming  english=92
xiaoming  chinese=98
xiaoming  math=89.5

3 case then 语句实例；
--简单case函数
case sex
  when '1' then '男'
  when '2' then '女’
  else '其他' end
--case搜索函数
case when sex = '1' then '男'
     when sex = '2' then '女'
     else '其他' end  

4 字符串操作方法
--字符串拼接
concat('foo', 'bar')  => 'foobar'
concat_ws(',','foo', 'bar') => 'foo,bar'
--返回index
instr(string str, string substr) 
0表示不包含，位置从1开始。
替换：replace（strA,strB） 
子串：substr(string|binary A, int start, int len) 
切分：split(string str, string pat)
大小写：upper(string A)，lower(string A) 
正则切分：

5 时间提取，年、月、日，系统时间戳获取，时间戳与日期格式转换；
获取日期：current_date
获取时间：current_timestamp
获取当前系统时间戳：unix_timestamp()
unix_timestamp()  获取时间戳；
unix_timestamp(string date, string pattern) 如unix_timestamp('2009-03-20', 'yyyy-MM-dd') = 1237532400. 
from_unixtime(bigint unixtime[, string format]) 时间戳转换为date类型，pattern如yyyy-MM-dd HH:mm:ss 

6 数据类型转换
cast(cast(a as string) as double)；



插入数据

其他函数：

insert  into /override
Sqoop 导入与导出



数据聚合分析部分：
这些函数中hive与Impala的区别。
常见的聚合函数，最大，最小，求和，平均，方差，标准差。
开窗函数topN，统计实例；
多种关联，leftjoin，join，inner join。
union 实例：union  与union all 
1 Windows 开窗函数
一般格式：
SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)
FROM T;
或者指定前后选取的行数：
UNBOUNDED PRECEDING指首行
UNBOUNDED FOLLOWING指尾行
CURRENT ROW当前行
3 PRECEDING 前3行 （不存在时将跳过）
3 FOLLOWING 后3行

2 与Windows 配合使用的分析函数：
RANK
ROW_NUMBER
DENSE_RANK
CUME_DIST
PERCENT_RANK

NTILE

3  取topN
lead(N) topN
lag(N)  tail N 






